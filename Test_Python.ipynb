{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40936546-eeae-4b6d-aeea-4ff1b48c5a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadr\n",
      "  Obtaining dependency information for pyreadr from https://files.pythonhosted.org/packages/44/1a/49c185be71926b768ebbd2242edc04ea2829e555e72055b768f61eb2fb8a/pyreadr-0.4.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pyreadr-0.4.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /opt/mamba/lib/python3.11/site-packages (from pyreadr) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/mamba/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadr) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/mamba/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadr) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/mamba/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadr) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadr) (1.16.0)\n",
      "Downloading pyreadr-0.4.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.7/435.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyreadr\n",
      "Successfully installed pyreadr-0.4.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073187d7-0e59-4a5e-b24d-abddafd24942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a3cf8b-3238-40c2-9432-f2e54160e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d783f5-3688-440e-9af0-72d9ff17f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996e6700-a223-43e5-ac79-ddb26fb6b9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gabriel1057d/Simulated_Data/.keep',\n",
       " 'gabriel1057d/Simulated_Data/AlarmeEpidemie0.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/AlarmeEpidemie10.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/AlarmeEpidemie2.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/AlarmeEpidemie3.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/AlarmeEpidemie5.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/AlarmesBaselines.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/BaseLines.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/BaseLines2.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/EpidemiesBaselines.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/EpidemiesSimulees0.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/EpidemiesSimulees10.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/EpidemiesSimulees2.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/EpidemiesSimulees3.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/EpidemiesSimulees5.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/SeriesOutbreak0.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/SeriesOutbreak10.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/SeriesOutbreak2.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/SeriesOutbreak3.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/SeriesOutbreak5.Rdata',\n",
       " 'gabriel1057d/Simulated_Data/simulated data.Rdata']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.ls(\"/gabriel1057d/Simulated_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae2f6c1-f569-4fb4-97e5-477320e7f662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class S3FileSystem in module s3fs.core:\n",
      "\n",
      "class S3FileSystem(fsspec.spec.AbstractFileSystem)\n",
      " |  S3FileSystem(*args, **kwargs)\n",
      " |  \n",
      " |  Access S3 as if it were a file system.\n",
      " |  \n",
      " |  This exposes a filesystem-like API (ls, cp, open, etc.) on top of S3\n",
      " |  storage.\n",
      " |  \n",
      " |  Provide credentials either explicitly (``key=``, ``secret=``) or depend\n",
      " |  on boto's credential methods. See botocore documentation for more\n",
      " |  information. If no credentials are available, use ``anon=True``.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  anon : bool (False)\n",
      " |      Whether to use anonymous connection (public buckets only). If False,\n",
      " |      uses the key/secret given, or boto's credential resolver (environment\n",
      " |      variables, config files, EC2 IAM server, in that order)\n",
      " |  key : string (None)\n",
      " |      If not anonymous, use this access key ID, if specified\n",
      " |  secret : string (None)\n",
      " |      If not anonymous, use this secret access key, if specified\n",
      " |  token : string (None)\n",
      " |      If not anonymous, use this security token, if specified\n",
      " |  use_ssl : bool (True)\n",
      " |      Whether to use SSL in connections to S3; may be faster without, but\n",
      " |      insecure\n",
      " |  s3_additional_kwargs : dict of parameters that are used when calling s3 api\n",
      " |      methods. Typically used for things like \"ServerSideEncryption\".\n",
      " |  client_kwargs : dict of parameters for the botocore client\n",
      " |  requester_pays : bool (False)\n",
      " |      If RequesterPays buckets are supported.\n",
      " |  default_block_size: int (None)\n",
      " |      If given, the default block size value used for ``open()``, if no\n",
      " |      specific value is given at all time. The built-in default is 5MB.\n",
      " |  default_fill_cache : Bool (True)\n",
      " |      Whether to use cache filling with open by default. Refer to\n",
      " |      ``S3File.open``.\n",
      " |  default_cache_type : string ('bytes')\n",
      " |      If given, the default cache_type value used for ``open()``. Set to \"none\"\n",
      " |      if no caching is desired. See fsspec's documentation for other available\n",
      " |      cache_type values. Default cache_type is 'bytes'.\n",
      " |  version_aware : bool (False)\n",
      " |      Whether to support bucket versioning.  If enable this will require the\n",
      " |      user to have the necessary IAM permissions for dealing with versioned\n",
      " |      objects.\n",
      " |  config_kwargs : dict of parameters passed to ``botocore.client.Config``\n",
      " |  kwargs : other parameters for core session\n",
      " |  session : botocore Session object to be used for all connections.\n",
      " |       This session will be used inplace of creating a new session inside S3FileSystem.\n",
      " |  \n",
      " |  The following parameters are passed on to fsspec:\n",
      " |  \n",
      " |  skip_instance_cache: to control reuse of instances\n",
      " |  use_listings_cache, listings_expiry_time, max_paths: to control reuse of directory listings\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> s3 = S3FileSystem(anon=False)  # doctest: +SKIP\n",
      " |  >>> s3.ls('my-bucket/')  # doctest: +SKIP\n",
      " |  ['my-file.txt']\n",
      " |  \n",
      " |  >>> with s3.open('my-bucket/my-file.txt', mode='rb') as f:  # doctest: +SKIP\n",
      " |  ...     print(f.read())  # doctest: +SKIP\n",
      " |  b'Hello, world!'\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      S3FileSystem\n",
      " |      fsspec.spec.AbstractFileSystem\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, anon=False, key=None, secret=None, token=None, use_ssl=True, client_kwargs=None, requester_pays=False, default_block_size=None, default_fill_cache=True, default_cache_type='bytes', version_aware=False, config_kwargs=None, s3_additional_kwargs=None, session=None, username=None, password=None, **kwargs)\n",
      " |      Create and configure file-system instance\n",
      " |      \n",
      " |      Instances may be cachable, so if similar enough arguments are seen\n",
      " |      a new instance is not required. The token attribute exists to allow\n",
      " |      implementations to cache instances if they wish.\n",
      " |      \n",
      " |      A reasonable default should be provided if there are no arguments.\n",
      " |      \n",
      " |      Subclasses should call this method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      use_listings_cache, listings_expiry_time, max_paths:\n",
      " |          passed to ``DirCache``, if the implementation supports\n",
      " |          directory listing caching. Pass use_listings_cache=False\n",
      " |          to disable such caching.\n",
      " |      skip_instance_cache: bool\n",
      " |          If this is a cachable implementation, pass True here to force\n",
      " |          creating a new instance even if a matching instance exists, and prevent\n",
      " |          storing this instance.\n",
      " |      asynchronous: bool\n",
      " |      loop: asyncio-compatible IOLoop or None\n",
      " |  \n",
      " |  bulk_delete(self, pathlist, **kwargs)\n",
      " |      Remove multiple keys with one call\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pathlist : listof strings\n",
      " |          The keys to remove, must all be in the same bucket.\n",
      " |  \n",
      " |  checksum(self, path, refresh=False)\n",
      " |      Unique value for current version of file\n",
      " |      \n",
      " |      If the checksum is the same from one moment to another, the contents\n",
      " |      are guaranteed to be the same. If the checksum changes, the contents\n",
      " |      *might* have changed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : string/bytes\n",
      " |          path of file to get checksum for\n",
      " |      refresh : bool (=False)\n",
      " |          if False, look in local cache for file details first\n",
      " |  \n",
      " |  chmod(self, path, acl, **kwargs)\n",
      " |      Set Access Control on a bucket/key\n",
      " |      \n",
      " |      See http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : string\n",
      " |          the object to set\n",
      " |      acl : string\n",
      " |          the value of ACL to apply\n",
      " |  \n",
      " |  connect(self, refresh=True)\n",
      " |      Establish S3 connection object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      refresh : bool\n",
      " |          Whether to create new session/client, even if a previous one with\n",
      " |          the same parameters already exists. If False (default), an\n",
      " |          existing one will be used if possible\n",
      " |  \n",
      " |  copy(self, path1, path2, **kwargs)\n",
      " |      Copy within two locations in the filesystem\n",
      " |      \n",
      " |      on_error : \"raise\", \"ignore\"\n",
      " |          If raise, any not-found exceptions will be raised; if ignore any\n",
      " |          not-found exceptions will cause the path to be skipped; defaults to\n",
      " |          raise unless recursive is true, where the default is ignore\n",
      " |  \n",
      " |  copy_basic(self, path1, path2, **kwargs)\n",
      " |      Copy file between locations on S3\n",
      " |      \n",
      " |      Not allowed where the origin is >5GB - use copy_managed\n",
      " |  \n",
      " |  copy_managed(self, path1, path2, block=5368709120, **kwargs)\n",
      " |      Copy file between locations on S3 as multi-part\n",
      " |      \n",
      " |      block: int\n",
      " |          The size of the pieces, must be larger than 5MB and at most 5GB.\n",
      " |          Smaller blocks mean more calls, only useful for testing.\n",
      " |  \n",
      " |  exists(self, path)\n",
      " |      Is there a file at the given path\n",
      " |  \n",
      " |  get_delegated_s3pars(self, exp=3600)\n",
      " |      Get temporary credentials from STS, appropriate for sending across a\n",
      " |      network. Only relevant where the key/secret were explicitly provided.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      exp : int\n",
      " |          Time in seconds that credentials are good for\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of parameters\n",
      " |  \n",
      " |  get_tags(self, path)\n",
      " |      Retrieve tag key/values for the given path\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      {str: str}\n",
      " |  \n",
      " |  getxattr(self, path, attr_name, **kwargs)\n",
      " |      Get an attribute from the metadata.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> mys3fs.getxattr('mykey', 'attribute_1')  # doctest: +SKIP\n",
      " |      'value_1'\n",
      " |  \n",
      " |  info(self, path, version_id=None, refresh=False)\n",
      " |      Give details of entry at path\n",
      " |      \n",
      " |      Returns a single dictionary, with exactly the same information as ``ls``\n",
      " |      would with ``detail=True``.\n",
      " |      \n",
      " |      The default implementation should calls ls and could be overridden by a\n",
      " |      shortcut. kwargs are passed on to ```ls()``.\n",
      " |      \n",
      " |      Some file systems might not be able to measure the file's size, in\n",
      " |      which case, the returned dict will include ``'size': None``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with keys: name (full path in the FS), size (in bytes), type (file,\n",
      " |      directory, or something else) and other FS-specific keys.\n",
      " |  \n",
      " |  invalidate_cache(self, path=None)\n",
      " |      Discard any cached directory information\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: string or None\n",
      " |          If None, clear all listings cached else listings at or under given\n",
      " |          path.\n",
      " |  \n",
      " |  isdir(self, path)\n",
      " |      Is this entry directory-like?\n",
      " |  \n",
      " |  ls(self, path, detail=False, refresh=False, **kwargs)\n",
      " |      List single \"directory\" with or without details\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : string/bytes\n",
      " |          location at which to list files\n",
      " |      detail : bool (=True)\n",
      " |          if True, each list item is a dict of file properties;\n",
      " |          otherwise, returns list of filenames\n",
      " |      refresh : bool (=False)\n",
      " |          if False, look in local cache for file details first\n",
      " |      kwargs : dict\n",
      " |          additional arguments passed on\n",
      " |  \n",
      " |  merge(self, path, filelist, **kwargs)\n",
      " |      Create single S3 file from list of S3 files\n",
      " |      \n",
      " |      Uses multi-part, no data is downloaded. The original files are\n",
      " |      not deleted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          The final file to produce\n",
      " |      filelist : list of str\n",
      " |          The paths, in order, to assemble into the final file.\n",
      " |  \n",
      " |  metadata(self, path, refresh=False, **kwargs)\n",
      " |      Return metadata of path.\n",
      " |      \n",
      " |      Metadata is cached unless `refresh=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : string/bytes\n",
      " |          filename to get metadata for\n",
      " |      refresh : bool (=False)\n",
      " |          if False, look in local cache for file metadata first\n",
      " |  \n",
      " |  mkdir(self, path, acl='', **kwargs)\n",
      " |      Create directory entry at path\n",
      " |      \n",
      " |      For systems that don't have true directories, may create an for\n",
      " |      this instance only and not touch the real filesystem\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: str\n",
      " |          location\n",
      " |      create_parents: bool\n",
      " |          if True, this is equivalent to ``makedirs``\n",
      " |      kwargs:\n",
      " |          may be permissions, etc.\n",
      " |  \n",
      " |  object_version_info(self, path, **kwargs)\n",
      " |  \n",
      " |  put_tags(self, path, tags, mode='o')\n",
      " |      Set tags for given existing key\n",
      " |      \n",
      " |      Tags are a str:str mapping that can be attached to any key, see\n",
      " |      https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocation-tag-restrictions.html\n",
      " |      \n",
      " |      This is similar to, but distinct from, key metadata, which is usually\n",
      " |      set at key creation time.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: str\n",
      " |          Existing key to attach tags to\n",
      " |      tags: dict str, str\n",
      " |          Tags to apply.\n",
      " |      mode:\n",
      " |          One of 'o' or 'm'\n",
      " |          'o': Will over-write any existing tags.\n",
      " |          'm': Will merge in new tags with existing tags.  Incurs two remote\n",
      " |          calls.\n",
      " |  \n",
      " |  rm(self, path, recursive=False, **kwargs)\n",
      " |      Remove keys and/or bucket.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : string\n",
      " |          The location to remove.\n",
      " |      recursive : bool (True)\n",
      " |          Whether to remove also all entries below, i.e., which are returned\n",
      " |          by `walk()`.\n",
      " |  \n",
      " |  rmdir(self, path)\n",
      " |      Remove a directory, if empty\n",
      " |  \n",
      " |  setxattr(self, path, copy_kwargs=None, **kw_args)\n",
      " |      Set metadata.\n",
      " |      \n",
      " |      Attributes have to be of the form documented in the\n",
      " |      `Metadata Reference`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      kw_args : key-value pairs like field=\"value\", where the values must be\n",
      " |          strings. Does not alter existing fields, unless\n",
      " |          the field appears here - if the value is None, delete the\n",
      " |          field.\n",
      " |      copy_kwargs : dict, optional\n",
      " |          dictionary of additional params to use for the underlying\n",
      " |          s3.copy_object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> mys3file.setxattr(attribute_1='value1', attribute_2='value2')  # doctest: +SKIP\n",
      " |      # Example for use with copy_args\n",
      " |      >>> mys3file.setxattr(copy_kwargs={'ContentType': 'application/pdf'},\n",
      " |      ...     attribute_1='value1')  # doctest: +SKIP\n",
      " |      \n",
      " |      \n",
      " |      .. Metadata Reference:\n",
      " |      http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-metadata\n",
      " |  \n",
      " |  split_path(self, path) -> Tuple[str, str, Optional[str]]\n",
      " |      Normalise S3 path string into bucket and key.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : string\n",
      " |          Input path, like `s3://mybucket/path/to/file`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> split_path(\"s3://mybucket/path/to/file\")\n",
      " |      ['mybucket', 'path/to/file', None]\n",
      " |      \n",
      " |      >>> split_path(\"s3://mybucket/path/to/versioned_file?versionId=some_version_id\")\n",
      " |      ['mybucket', 'path/to/versioned_file', 'some_version_id']\n",
      " |  \n",
      " |  touch(self, path, truncate=True, data=None, **kwargs)\n",
      " |      Create empty file or truncate\n",
      " |  \n",
      " |  url(self, path, expires=3600, **kwargs)\n",
      " |      Generate presigned URL to access path by HTTP\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : string\n",
      " |          the key path we are interested in\n",
      " |      expires : int\n",
      " |          the number of seconds this signature will be good for.\n",
      " |  \n",
      " |  walk(self, path, maxdepth=None, **kwargs)\n",
      " |      Return all files belows path\n",
      " |      \n",
      " |      List all files, recursing into subdirectories; output is iterator-style,\n",
      " |      like ``os.walk()``. For a simple list of files, ``find()`` is available.\n",
      " |      \n",
      " |      When topdown is True, the caller can modify the dirnames list in-place (perhaps\n",
      " |      using del or slice assignment), and walk() will\n",
      " |      only recurse into the subdirectories whose names remain in dirnames;\n",
      " |      this can be used to prune the search, impose a specific order of visiting,\n",
      " |      or even to inform walk() about directories the caller creates or renames before\n",
      " |      it resumes walk() again.\n",
      " |      Modifying dirnames when topdown is False has no effect. (see os.walk)\n",
      " |      \n",
      " |      Note that the \"files\" outputted will include anything that is not\n",
      " |      a directory, such as links.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: str\n",
      " |          Root to recurse into\n",
      " |      maxdepth: int\n",
      " |          Maximum recursion depth. None means limitless, but not recommended\n",
      " |          on link-based file-systems.\n",
      " |      topdown: bool (True)\n",
      " |          Whether to walk the directory tree from the top downwards or from\n",
      " |          the bottom upwards.\n",
      " |      kwargs: passed to ``ls``\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  connect_timeout = 5\n",
      " |  \n",
      " |  default_block_size = 5242880\n",
      " |  \n",
      " |  protocol = ['s3', 's3a']\n",
      " |  \n",
      " |  read_timeout = 15\n",
      " |  \n",
      " |  retries = 5\n",
      " |  \n",
      " |  root_marker = ''\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from fsspec.spec.AbstractFileSystem:\n",
      " |  \n",
      " |  __dask_tokenize__(self)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  cat(self, path, recursive=False, on_error='raise', **kwargs)\n",
      " |      Fetch (potentially multiple) paths' contents\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      recursive: bool\n",
      " |          If True, assume the path(s) are directories, and get all the\n",
      " |          contained files\n",
      " |      on_error : \"raise\", \"omit\", \"return\"\n",
      " |          If raise, an underlying exception will be raised (converted to KeyError\n",
      " |          if the type is in self.missing_exceptions); if omit, keys with exception\n",
      " |          will simply not be included in the output; if \"return\", all keys are\n",
      " |          included in the output, but the value will be bytes or an exception\n",
      " |          instance.\n",
      " |      kwargs: passed to cat_file\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of {path: contents} if there are multiple paths\n",
      " |      or the path has been otherwise expanded\n",
      " |  \n",
      " |  cat_file(self, path, start=None, end=None, **kwargs)\n",
      " |      Get the content of a file\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: URL of file on this filesystems\n",
      " |      start, end: int\n",
      " |          Bytes limits of the read. If negative, backwards from end,\n",
      " |          like usual python slices. Either can be None for start or\n",
      " |          end of file, respectively\n",
      " |      kwargs: passed to ``open()``.\n",
      " |  \n",
      " |  cat_ranges(self, paths, starts, ends, max_gap=None, on_error='return', **kwargs)\n",
      " |  \n",
      " |  cp(self, path1, path2, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.copy`.\n",
      " |  \n",
      " |  cp_file(self, path1, path2, **kwargs)\n",
      " |  \n",
      " |  created(self, path)\n",
      " |      Return the created timestamp of a file as a datetime.datetime\n",
      " |  \n",
      " |  delete(self, path, recursive=False, maxdepth=None)\n",
      " |      Alias of `AbstractFileSystem.rm`.\n",
      " |  \n",
      " |  disk_usage(self, path, total=True, maxdepth=None, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.du`.\n",
      " |  \n",
      " |  download(self, rpath, lpath, recursive=False, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.get`.\n",
      " |  \n",
      " |  du(self, path, total=True, maxdepth=None, withdirs=False, **kwargs)\n",
      " |      Space used by files and optionally directories within a path\n",
      " |      \n",
      " |      Directory size does not include the size of its contents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: str\n",
      " |      total: bool\n",
      " |          Whether to sum all the file sizes\n",
      " |      maxdepth: int or None\n",
      " |          Maximum number of directory levels to descend, None for unlimited.\n",
      " |      withdirs: bool\n",
      " |          Whether to include directory paths in the output.\n",
      " |      kwargs: passed to ``find``\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dict of {path: size} if total=False, or int otherwise, where numbers\n",
      " |      refer to bytes used.\n",
      " |  \n",
      " |  end_transaction(self)\n",
      " |      Finish write transaction, non-context version\n",
      " |  \n",
      " |  expand_path(self, path, recursive=False, maxdepth=None, **kwargs)\n",
      " |      Turn one or more globs or directories into a list of all matching paths\n",
      " |      to files or directories.\n",
      " |      \n",
      " |      kwargs are passed to ``glob`` or ``find``, which may in turn call ``ls``\n",
      " |  \n",
      " |  find(self, path, maxdepth=None, withdirs=False, detail=False, **kwargs)\n",
      " |      List all files below path.\n",
      " |      \n",
      " |      Like posix ``find`` command without conditions\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |      maxdepth: int or None\n",
      " |          If not None, the maximum number of levels to descend\n",
      " |      withdirs: bool\n",
      " |          Whether to include directory paths in the output. This is True\n",
      " |          when used by glob, but users usually only want files.\n",
      " |      kwargs are passed to ``ls``.\n",
      " |  \n",
      " |  get(self, rpath, lpath, recursive=False, callback=<fsspec.callbacks.NoOpCallback object at 0x7f02fa43f8d0>, maxdepth=None, **kwargs)\n",
      " |      Copy file(s) to local.\n",
      " |      \n",
      " |      Copies a specific file or tree of files (if recursive=True). If lpath\n",
      " |      ends with a \"/\", it will be assumed to be a directory, and target files\n",
      " |      will go within. Can submit a list of paths, which may be glob-patterns\n",
      " |      and will be expanded.\n",
      " |      \n",
      " |      Calls get_file for each source.\n",
      " |  \n",
      " |  get_file(self, rpath, lpath, callback=<fsspec.callbacks.NoOpCallback object at 0x7f02fa43f8d0>, outfile=None, **kwargs)\n",
      " |      Copy single remote file to local\n",
      " |  \n",
      " |  get_mapper(self, root='', check=False, create=False, missing_exceptions=None)\n",
      " |      Create key/value store based on this file-system\n",
      " |      \n",
      " |      Makes a MutableMapping interface to the FS at the given root path.\n",
      " |      See ``fsspec.mapping.FSMap`` for further details.\n",
      " |  \n",
      " |  glob(self, path, **kwargs)\n",
      " |      Find files by glob-matching.\n",
      " |      \n",
      " |      If the path ends with '/' and does not contain \"*\", it is essentially\n",
      " |      the same as ``ls(path)``, returning only files.\n",
      " |      \n",
      " |      We support ``\"**\"``,\n",
      " |      ``\"?\"`` and ``\"[..]\"``. We do not support ^ for pattern negation.\n",
      " |      \n",
      " |      Search path names that contain embedded characters special to this\n",
      " |      implementation of glob may not produce expected results;\n",
      " |      e.g., 'foo/bar/*starredfilename*'.\n",
      " |      \n",
      " |      kwargs are passed to ``ls``.\n",
      " |  \n",
      " |  head(self, path, size=1024)\n",
      " |      Get the first ``size`` bytes from file\n",
      " |  \n",
      " |  isfile(self, path)\n",
      " |      Is this entry file-like?\n",
      " |  \n",
      " |  lexists(self, path, **kwargs)\n",
      " |      If there is a file at the given path (including\n",
      " |      broken links)\n",
      " |  \n",
      " |  listdir(self, path, detail=True, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.ls`.\n",
      " |  \n",
      " |  makedir(self, path, create_parents=True, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.mkdir`.\n",
      " |  \n",
      " |  makedirs(self, path, exist_ok=False)\n",
      " |      Recursively make directories\n",
      " |      \n",
      " |      Creates directory at path and any intervening required directories.\n",
      " |      Raises exception if, for instance, the path already exists but is a\n",
      " |      file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: str\n",
      " |          leaf directory name\n",
      " |      exist_ok: bool (False)\n",
      " |          If False, will error if the target already exists\n",
      " |  \n",
      " |  mkdirs(self, path, exist_ok=False)\n",
      " |      Alias of `AbstractFileSystem.makedirs`.\n",
      " |  \n",
      " |  modified(self, path)\n",
      " |      Return the modified timestamp of a file as a datetime.datetime\n",
      " |  \n",
      " |  move(self, path1, path2, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.mv`.\n",
      " |  \n",
      " |  mv(self, path1, path2, recursive=False, maxdepth=None, **kwargs)\n",
      " |      Move file(s) from one location to another\n",
      " |  \n",
      " |  open(self, path, mode='rb', block_size=None, cache_options=None, compression=None, **kwargs)\n",
      " |      Return a file-like object from the filesystem\n",
      " |      \n",
      " |      The resultant instance must function correctly in a context ``with``\n",
      " |      block.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: str\n",
      " |          Target file\n",
      " |      mode: str like 'rb', 'w'\n",
      " |          See builtin ``open()``\n",
      " |      block_size: int\n",
      " |          Some indication of buffering - this is a value in bytes\n",
      " |      cache_options : dict, optional\n",
      " |          Extra arguments to pass through to the cache.\n",
      " |      compression: string or None\n",
      " |          If given, open file using compression codec. Can either be a compression\n",
      " |          name (a key in ``fsspec.compression.compr``) or \"infer\" to guess the\n",
      " |          compression from the filename suffix.\n",
      " |      encoding, errors, newline: passed on to TextIOWrapper for text mode\n",
      " |  \n",
      " |  pipe(self, path, value=None, **kwargs)\n",
      " |      Put value into path\n",
      " |      \n",
      " |      (counterpart to ``cat``)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: string or dict(str, bytes)\n",
      " |          If a string, a single remote location to put ``value`` bytes; if a dict,\n",
      " |          a mapping of {path: bytesvalue}.\n",
      " |      value: bytes, optional\n",
      " |          If using a single path, these are the bytes to put there. Ignored if\n",
      " |          ``path`` is a dict\n",
      " |  \n",
      " |  pipe_file(self, path, value, **kwargs)\n",
      " |      Set the bytes of given file\n",
      " |  \n",
      " |  put(self, lpath, rpath, recursive=False, callback=<fsspec.callbacks.NoOpCallback object at 0x7f02fa43f8d0>, maxdepth=None, **kwargs)\n",
      " |      Copy file(s) from local.\n",
      " |      \n",
      " |      Copies a specific file or tree of files (if recursive=True). If rpath\n",
      " |      ends with a \"/\", it will be assumed to be a directory, and target files\n",
      " |      will go within.\n",
      " |      \n",
      " |      Calls put_file for each source.\n",
      " |  \n",
      " |  put_file(self, lpath, rpath, callback=<fsspec.callbacks.NoOpCallback object at 0x7f02fa43f8d0>, **kwargs)\n",
      " |      Copy single file to remote\n",
      " |  \n",
      " |  read_block(self, fn, offset, length, delimiter=None)\n",
      " |      Read a block of bytes from\n",
      " |      \n",
      " |      Starting at ``offset`` of the file, read ``length`` bytes.  If\n",
      " |      ``delimiter`` is set then we ensure that the read starts and stops at\n",
      " |      delimiter boundaries that follow the locations ``offset`` and ``offset\n",
      " |      + length``.  If ``offset`` is zero then we start at zero.  The\n",
      " |      bytestring returned WILL include the end delimiter string.\n",
      " |      \n",
      " |      If offset+length is beyond the eof, reads to eof.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fn: string\n",
      " |          Path to filename\n",
      " |      offset: int\n",
      " |          Byte offset to start read\n",
      " |      length: int\n",
      " |          Number of bytes to read. If None, read to end.\n",
      " |      delimiter: bytes (optional)\n",
      " |          Ensure reading starts and stops at delimiter bytestring\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> fs.read_block('data/file.csv', 0, 13)  # doctest: +SKIP\n",
      " |      b'Alice, 100\\nBo'\n",
      " |      >>> fs.read_block('data/file.csv', 0, 13, delimiter=b'\\n')  # doctest: +SKIP\n",
      " |      b'Alice, 100\\nBob, 200\\n'\n",
      " |      \n",
      " |      Use ``length=None`` to read to the end of the file.\n",
      " |      >>> fs.read_block('data/file.csv', 0, None, delimiter=b'\\n')  # doctest: +SKIP\n",
      " |      b'Alice, 100\\nBob, 200\\nCharlie, 300'\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`fsspec.utils.read_block`\n",
      " |  \n",
      " |  read_bytes(self, path, start=None, end=None, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.cat_file`.\n",
      " |  \n",
      " |  read_text(self, path, encoding=None, errors=None, newline=None, **kwargs)\n",
      " |      Get the contents of the file as a string.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: str\n",
      " |          URL of file on this filesystems\n",
      " |      encoding, errors, newline: same as `open`.\n",
      " |  \n",
      " |  rename(self, path1, path2, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.mv`.\n",
      " |  \n",
      " |  rm_file(self, path)\n",
      " |      Delete a file\n",
      " |  \n",
      " |  sign(self, path, expiration=100, **kwargs)\n",
      " |      Create a signed URL representing the given path\n",
      " |      \n",
      " |      Some implementations allow temporary URLs to be generated, as a\n",
      " |      way of delegating credentials.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |           The path on the filesystem\n",
      " |      expiration : int\n",
      " |          Number of seconds to enable the URL for (if supported)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      URL : str\n",
      " |          The signed URL\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      NotImplementedError : if method is not implemented for a filesystem\n",
      " |  \n",
      " |  size(self, path)\n",
      " |      Size in bytes of file\n",
      " |  \n",
      " |  sizes(self, paths)\n",
      " |      Size in bytes of each file in a list of paths\n",
      " |  \n",
      " |  start_transaction(self)\n",
      " |      Begin write transaction for deferring files, non-context version\n",
      " |  \n",
      " |  stat(self, path, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.info`.\n",
      " |  \n",
      " |  tail(self, path, size=1024)\n",
      " |      Get the last ``size`` bytes from file\n",
      " |  \n",
      " |  to_json(self)\n",
      " |      JSON representation of this filesystem instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str: JSON structure with keys cls (the python location of this class),\n",
      " |          protocol (text name of this class's protocol, first one in case of\n",
      " |          multiple), args (positional args, usually empty), and all other\n",
      " |          kwargs as their own keys.\n",
      " |  \n",
      " |  ukey(self, path)\n",
      " |      Hash of file properties, to tell if it has changed\n",
      " |  \n",
      " |  unstrip_protocol(self, name)\n",
      " |      Format FS-specific path to generic, including protocol\n",
      " |  \n",
      " |  upload(self, lpath, rpath, recursive=False, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.put`.\n",
      " |  \n",
      " |  write_bytes(self, path, value, **kwargs)\n",
      " |      Alias of `AbstractFileSystem.pipe_file`.\n",
      " |  \n",
      " |  write_text(self, path, value, encoding=None, errors=None, newline=None, **kwargs)\n",
      " |      Write the text to the given file.\n",
      " |      \n",
      " |      An existing file will be overwritten.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path: str\n",
      " |          URL of file on this filesystems\n",
      " |      value: str\n",
      " |          Text to write.\n",
      " |      encoding, errors, newline: same as `open`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from fsspec.spec.AbstractFileSystem:\n",
      " |  \n",
      " |  clear_instance_cache() from fsspec.spec._Cached\n",
      " |      Clear the cache of filesystem instances.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unless overridden by setting the ``cachable`` class attribute to False,\n",
      " |      the filesystem class stores a reference to newly created instances. This\n",
      " |      prevents Python's normal rules around garbage collection from working,\n",
      " |      since the instances refcount will not drop to zero until\n",
      " |      ``clear_instance_cache`` is called.\n",
      " |  \n",
      " |  current() from fsspec.spec._Cached\n",
      " |      Return the most recently instantiated FileSystem\n",
      " |      \n",
      " |      If no instance has been created, then create one with defaults\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from fsspec.spec.AbstractFileSystem:\n",
      " |  \n",
      " |  from_json(blob)\n",
      " |      Recreate a filesystem instance from JSON representation\n",
      " |      \n",
      " |      See ``.to_json()`` for the expected structure of the input\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      blob: str\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      file system instance, not necessarily of this particular class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from fsspec.spec.AbstractFileSystem:\n",
      " |  \n",
      " |  fsid\n",
      " |      Persistent filesystem id that can be used to compare filesystems\n",
      " |      across sessions.\n",
      " |  \n",
      " |  transaction\n",
      " |      A context within which files are committed together upon exit\n",
      " |      \n",
      " |      Requires the file class to implement `.commit()` and `.discard()`\n",
      " |      for the normal and exception cases.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from fsspec.spec.AbstractFileSystem:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from fsspec.spec.AbstractFileSystem:\n",
      " |  \n",
      " |  async_impl = False\n",
      " |  \n",
      " |  blocksize = 4194304\n",
      " |  \n",
      " |  cachable = True\n",
      " |  \n",
      " |  mirror_sync_methods = False\n",
      " |  \n",
      " |  sep = '/'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(s3fs.S3FileSystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e46867b-298c-4d9b-bdb6-ccbdf6e3bcbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'S3File' object has no attribute 'pyreadr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m FILE_PATH_S3 \u001b[38;5;241m=\u001b[39m BUCKET \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m FILE_KEY_S3\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(FILE_PATH_S3, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_in:\n\u001b[0;32m----> 6\u001b[0m     Valid_data \u001b[38;5;241m=\u001b[39m \u001b[43mfile_in\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyreadr\u001b[49m\u001b[38;5;241m.\u001b[39mread_r()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'S3File' object has no attribute 'pyreadr'"
     ]
    }
   ],
   "source": [
    "BUCKET = \"gabriel1057d/Simulated_Data\"\n",
    "FILE_KEY_S3 = \"SeriesOutbreak0.Rdata\"\n",
    "FILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n",
    "\n",
    "# with fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n",
    "#     Valid_data = file_in.pyreadr.read_r()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2fcd3d7-1a30-4228-beb6-0116dbaa625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid_data = pyreadr.read_r('s3://gabriel1057d/Simulated_Data/SeriesOutbreak0.Rdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fde51f52-a1c8-42da-a579-2cc99caea659",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not S3File",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Valid_data \u001b[38;5;241m=\u001b[39m \u001b[43mpyreadr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_r\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILE_PATH_S3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pyreadr/pyreadr.py:45\u001b[0m, in \u001b[0;36mread_r\u001b[0;34m(path, use_objects, timezone)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(os, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfsencode\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m         filename_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfsencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile path could not be encoded with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m which is set as your system encoding, trying to encode it as utf-8. Please set your system encoding correctly.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetfilesystemencoding())\n",
      "File \u001b[0;32m<frozen os>:812\u001b[0m, in \u001b[0;36mfsencode\u001b[0;34m(filename)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not S3File"
     ]
    }
   ],
   "source": [
    "Valid_data = pyreadr.read_r(fs.open(FILE_PATH_S3, mode=\"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
